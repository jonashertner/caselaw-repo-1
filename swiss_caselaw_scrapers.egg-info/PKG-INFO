Metadata-Version: 2.4
Name: swiss-caselaw-scrapers
Version: 0.1.0
Summary: Scraping framework for Swiss federal and cantonal court decisions
Author: Jonas Hertner
License-Expression: MIT
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: requests>=2.31
Requires-Dist: beautifulsoup4>=4.12
Requires-Dist: lxml>=4.9
Requires-Dist: pydantic>=2.0
Requires-Dist: pyarrow>=14.0
Requires-Dist: huggingface-hub>=0.20
Provides-Extra: crypto
Requires-Dist: pycryptodome>=3.20; extra == "crypto"
Provides-Extra: pdf
Requires-Dist: pymupdf>=1.23; extra == "pdf"
Provides-Extra: api
Requires-Dist: fastapi>=0.104; extra == "api"
Requires-Dist: uvicorn>=0.24; extra == "api"
Provides-Extra: dev
Requires-Dist: pytest>=7.0; extra == "dev"
Requires-Dist: ruff>=0.1; extra == "dev"
Provides-Extra: all
Requires-Dist: swiss-caselaw-scrapers[api,crypto,dev,pdf]; extra == "all"
Dynamic: license-file

# Swiss Caselaw

Open infrastructure for Swiss court decisions — scraping, search, and analysis.

**Goal:** A comprehensive, daily-updated, freely accessible dataset of all published Swiss court decisions (federal and cantonal), with intelligent full-text search for practitioners and bulk access for researchers.

## Current Status

**Phase 1 complete.** The scraping framework and all federal court scrapers are built and validated against live servers (Feb 2026). No dataset has been published yet — that requires running the scrapers over historical data, which is Phase 3.

### What exists today

- Unified scraping framework (models, pipeline, state management, rate limiting)
- 5 federal court scrapers, validated against live endpoints:

| Court | Decisions | Coverage | Platform | Status |
|-------|-----------|----------|----------|--------|
| **BGer** — Federal Supreme Court | ~250,000 | 2000–present | Eurospider (SHA-256 PoW) | ✅ Validated, 30/31 tests |
| **BGE** — Published leading cases | ~15,000 | 1954–present | Eurospider CLIR | ✅ Built |
| **BVGer** — Federal Administrative Court | ~50,000 | 2007–present | Weblaw API + jurispub fallback | ✅ Rewritten, dual-mode |
| **BStGer** — Federal Criminal Court | ~5,000 | 2005–present | Weblaw JSON API | ✅ API confirmed live |
| **BPatGer** — Federal Patent Court | ~250 | 2012–present | TYPO3 HTML | ✅ Selectors verified |

- Cantonal court registry (all 26 cantons mapped by platform type)
- Three cantonal base classes covering ~90% of cantonal platforms
- One cantonal scraper implemented (ZH Obergericht)

### What does not exist yet

- The remaining ~24 cantonal scrapers
- The actual dataset (requires multi-day historical scraping runs)
- HuggingFace dataset publication
- MCP server for practitioner access
- Daily automation (GitHub Actions)

---

## Roadmap

### Phase 1 — Framework + Federal Courts ✅

Build the core infrastructure and scrapers for all 4 federal courts plus BGE.

### Phase 2 — Cantonal Courts

Implement scrapers for all 26 cantons. Three platform base classes already handle the heavy lifting:

| Platform | Base Class | Cantons | Effort |
|----------|-----------|---------|--------|
| Weblaw (query_ticket) | `base_weblaw.py` | AG, ZG, NW, OW, LU, SG, FR, AR, GR, JU, AI, SZ, VS, BL | ~10 lines config each |
| Tribuna (GWT-RPC) | `base_tribuna.py` | TI, GE, BS, NE, GL, SO, VD, SH | ~10 lines config each |
| Vaadin | `base_vaadin.py` | BE, TG, partial ZH, SH | ~10 lines config each |
| Custom | individual | ZH (VerwG), UR | ~100 lines each |

Most cantonal scrapers are thin configuration layers over the base classes. The registry in `scrapers/cantonal/registry.py` has URLs, platform types, and coverage dates for every canton.

### Phase 3 — Historical Backfill + Dataset Publication

Run all scrapers over their full historical range. Estimated corpus:

| Source | Est. decisions | Time range |
|--------|---------------|------------|
| Federal courts | ~320,000 | 1954–2026 |
| Cantonal courts | ~500,000+ | varies |
| **Total** | **~800,000+** | |

Publish as a Parquet dataset on HuggingFace Hub with monthly shards, versioned releases, and a standardised schema. The pipeline already supports incremental Parquet export and shard management.

### Phase 4 — Daily Updates + Search

- **GitHub Actions** workflow: nightly scrape → incremental Parquet → HuggingFace push
- **FTS5 full-text search** index (SQLite) for local deployment
- **MCP server** exposing the dataset to Claude and other LLM tools:
  - `search_decisions(query, court, date_range, language)` — semantic + keyword search
  - `get_decision(decision_id)` — full text + metadata
  - `cite_check(docket_number)` — find all decisions citing a given ruling
  - `court_statistics(court, year)` — aggregate analytics

The MCP server is the highest-value access layer for practitioners — it lets you query Swiss caselaw directly from Claude, Cursor, or any MCP-compatible tool.

### Phase 5 — Research Access + Analysis

- Bulk download via HuggingFace (`datasets` library, Parquet, or CSV)
- Citation graph (which decisions cite which, across all courts)
- Judicial analytics (outcome rates by court/chamber/year, processing times, language distribution)
- Judge/clerk extraction and panel composition analysis

---

## Architecture

```
pipeline.py                  ← Orchestrator: scrape → Parquet → HuggingFace → FTS5
├── base_scraper.py          ← Abstract base (rate limiting, state, sessions)
├── models.py                ← Unified Decision schema (Pydantic)
├── scrapers/
│   ├── bger.py              ← BGer (SHA-256 PoW, AZA search, 4-day windowing)
│   ├── bge.py               ← BGE Leitentscheide (CLIR, volumes I-V, trilingual)
│   ├── bvger.py             ← BVGer (Weblaw API primary, jurispub.admin.ch fallback)
│   ├── bstger.py            ← BStGer (Weblaw JSON API, adaptive date windows)
│   ├── bpatger.py           ← BPatGer (TYPO3 listing pages + detail scraping)
│   └── cantonal/
│       ├── registry.py      ← All 26 cantons: platform, URLs, coverage
│       ├── base_weblaw.py   ← Weblaw query_ticket base (14 cantons)
│       ├── base_tribuna.py  ← Tribuna GWT-RPC base (8 cantons)
│       ├── base_vaadin.py   ← Vaadin base (4 cantons)
│       └── zh_obergericht.py
├── fts5_server.py           ← Full-text search server (placeholder)
├── test_parsing.py          ← Offline parsing tests
├── test_bger_live.py        ← BGer live validation (31 tests)
├── test_federal_live.py     ← BVGer/BStGer/BPatGer live validation
├── requirements.txt
└── pyproject.toml
```

## Data Model

Every scraper produces a `Decision` object with this schema:

| Field | Type | Description |
|-------|------|-------------|
| `decision_id` | str | Unique key: `{court}_{docket_normalized}` |
| `court` | str | `bger`, `bvger`, `bstger`, `bpatger`, `zh_obergericht`, ... |
| `canton` | str | `CH` (federal) or two-letter cantonal code |
| `chamber` | str? | Court division / Abteilung |
| `docket_number` | str | Original format: `6B_1234/2025`, `A-668/2020` |
| `decision_date` | date | Date of the ruling |
| `publication_date` | date? | When published online |
| `language` | str | `de`, `fr`, `it`, `rm` |
| `full_text` | str | Complete decision text |
| `title` | str? | Short title or Stichwort |
| `regeste` | str? | Headnote / Regeste |
| `abstract_de`, `abstract_fr`, `abstract_it` | str? | Trilingual abstracts (BGE only) |
| `legal_area` | str? | Rechtsgebiet |
| `decision_type` | str? | Urteil, Beschluss, Verfügung, ... |
| `outcome` | str? | Gutheissung, Abweisung, Nichteintreten, ... |
| `judges` | str? | Panel composition |
| `clerks` | str? | Gerichtsschreiber |
| `collection` | str? | BGE reference if published as leading case |
| `appeal_info` | str? | Appeal status, BGer reference |
| `cited_decisions` | list[str] | Extracted BGE and docket references |
| `source_url` | str | Permanent link to original |
| `pdf_url` | str? | Direct PDF download link |
| `scraped_at` | datetime | When scraped |

## Quick Start

```bash
git clone https://github.com/jonashertner/caselaw-repo.git
cd caselaw-repo
pip install -e ".[all]"

# Smoke test: scrape 5 recent BGer decisions
python pipeline.py --scrape --courts bger --max 5 -v

# Run live validation tests
python test_federal_live.py

# Scrape specific courts with date filter
python pipeline.py --scrape --courts bger,bstger,bvger --since 2026-01-01 --max 20

# Individual scraper with debug output
python scrapers/bger.py --max 3 -v
python scrapers/bstger.py --since 2026-01-01 --max 5 -v
python scrapers/bvger.py --since 2026-01-01 --max 5 -v
python scrapers/bpatger.py --max 5 -v
```

## Technical Notes

**BGer Proof-of-Work.** The Federal Supreme Court's Eurospider platform requires a SHA-256 proof-of-work cookie. Without it, requests redirect to `pow.php`. The scraper mines a nonce where `SHA256(fingerprint + nonce)` has 16 leading zero bits (~65k hashes, under 1 second). This is only needed for `search.bger.ch`, not for `relevancy.bger.ch` direct fetches.

**BVGer dual-mode.** BVGer migrated from ICEfaces (jurispub.admin.ch) to Weblaw Lawsearch v4 (bvger.weblaw.ch) in 2023. Both platforms remain active as of Feb 2026. The scraper tries the Weblaw JSON API first and falls back to ICEfaces automatically. The old platform may be shut down without notice.

**Rate limiting.** All scrapers enforce a minimum 3-second delay between requests. BVGer and BStGer use adaptive date windowing (start with 64-day ranges, halve if results exceed 100) to avoid overwhelming the APIs.

**Court decisions are public records** under Swiss law. The Bundesgericht has consistently held that court decisions must be made accessible to the public (BGE 133 I 106, BGE 139 I 129). This project scrapes only publicly available, officially published decisions.

## Contributing

Contributions are welcome, particularly:

- Cantonal scraper implementations (most are ~10 lines of config over the base classes)
- Test coverage for existing scrapers
- MCP server implementation
- Data quality improvements (better outcome detection, judge extraction, citation parsing)

See `scrapers/cantonal/registry.py` for which cantons still need scrapers.

## License

MIT. See [LICENSE](LICENSE).
